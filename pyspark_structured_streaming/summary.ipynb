{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the SparkSession\n",
    "spark = SparkSession\n",
    " .builder\n",
    " .appName(\"Spark-Kafka-Integration\")\n",
    " .master(\"local\")\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Schema\n",
    "val mySchema = StructType(Array(\n",
    " StructField(\"id\", IntegerType),\n",
    " StructField(\"name\", StringType),\n",
    " StructField(\"year\", IntegerType),\n",
    " StructField(\"rating\", DoubleType),\n",
    " StructField(\"duration\", IntegerType)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Streaming Dataframe\n",
    "streamingDataFrame = spark.readStream.schema(mySchema).csv(\"path of your directory like home/Desktop/dir/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the stream  to Kafka\n",
    "streamingDataFrame.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\").writeStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"topic\", \"topicName\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "  .option(\"checkpointLocation\", \"path to your local dir\")\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subscribe the stream from Kafka\n",
    "import spark.implicits._\n",
    "df = spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "  .option(\"subscribe\", \"topicName\")\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Stream according to my schema along with TimeStamp\n",
    "df1 = df.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS TIMESTAMP)\").as[(String, Timestamp)]\n",
    "  .select(from_json($\"value\", mySchema).as(\"data\"), $\"timestamp\")\n",
    "  .select(\"data.*\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we just print our data to the console.\n",
    "\n",
    "df1.writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\",\"false\")\n",
    "    .start()\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Streaming \n",
    "##### https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "##### https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n",
    "It was was introduced in Spark 2.0 (and became stable in 2.2) as an extension built on top of Spark SQL. Because of that, it takes advantage of Spark SQL code and memory optimizations. Structured Streaming also gives very powerful abstractions like Dataset/DataFrame APIs as well as SQL. No more dealing with RDD directly! It uses micro-batches to process data streams as a series of small-batch jobs with low latency i.e You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Kafka Source for Streaming Queries\n",
    "# Subscribe to 1 topic ( for multiple topics - .option(\"subscribe\", \"topic1,topic2\") \\)\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\ \n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Kafka Source for Batch Queries \n",
    "#Subscribe to multiple topics, specifying explicit Kafka offsets. Deafult values for startingOffsets & endingOffsets are \"earliest\" & \"latest\" respectively.\n",
    "val df = spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n",
    "  .option(\"subscribe\", \"topic1,topic2\")\n",
    "  .option(\"startingOffsets\", \"\"\"{\"topic1\":{\"0\":23,\"1\":-2},\"topic2\":{\"0\":-2}}\"\"\")\n",
    "  .option(\"endingOffsets\", \"\"\"{\"topic1\":{\"0\":50,\"1\":-1},\"topic2\":{\"0\":-1}}\"\"\")\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "  .as[(String, String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Kafka Sink for Streaming Queries\n",
    "# Write key-value data from a DataFrame to a specific Kafka topic specified in an option.\n",
    "ds = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"topic\", \"topic1\") \\\n",
    "  .start()\n",
    "\n",
    "# Write key-value data from a DataFrame to Kafka using a topic specified in the data\n",
    "ds = df \\\n",
    "  .selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the output of Batch Queries to Kafka\n",
    "# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .write \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"topic\", \"topic1\") \\\n",
    "  .save()\n",
    "\n",
    "# Write key-value data from a DataFrame to Kafka using a topic specified in the data\n",
    "df.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .write \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above code is written in file spark_test.py and topic is meetup. The above program submitted using below command\n",
    "#./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 spark_test.py localhost:2181 meetup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Streaming - using DStreams\n",
    "##### https://spark.apache.org/docs/2.1.1/streaming-kafka-0-8-integration.html\n",
    "Spark Streaming went alpha with Spark 0.7.0. It’s based on the idea of discretized streams or DStreams. Each DStream is represented as a sequence of RDDs, so it’s easy to use if you’re coming from low-level RDD-backed batch workloads. DStreams underwent a lot of improvements over that period of time, but there were still various challenges, primarily because it’s a very low-level API. Since DStream is just a collection of RDDs, it’s typically used for low-level transformations and processing. Adding a DataFrames API on top of that provides very powerful abstractions like SQL, but requires a bit more configuration. And if you have a simple use case, Spark Structured Streaming might be a better solution in general!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Sparksession\n",
    "\n",
    "\n",
    "#Creaitng spark Context since we are submitting from the command line. This is basucally a receiver code\n",
    "'''\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"MeetupStreaming\") \\\n",
    "        .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "sc = SparkContext(appName=\"MeetupStreaming\")\n",
    "sc.setLogLevel(\"FATAL\")\n",
    "ssc = StreamingContext(sc, 60)\n",
    "'''\n",
    "\n",
    "sc = SparkContext(\"local[*]\",appName=\"MeetupStreaming\")\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "#Creating the streaming context\n",
    "ssc = StreamingContext(sc, batchDuration=60)\n",
    "\n",
    "kafkaStream = KafkaUtils.createStream(ssc, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume])\n",
    "'''\n",
    "brokers='localhost:9092'\n",
    "topic='tweets'\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, topics=[topic], kafkaParams={\"metadata.broker.list\": brokers}) # for receiver-less “direct” approach.\n",
    "'''\n",
    "\n",
    "# The above code is written in file spark_test.py and topic is meetup. The above program submitted using below command\n",
    "# ./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.1 spark_test.py localhost:2181 meetup"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
